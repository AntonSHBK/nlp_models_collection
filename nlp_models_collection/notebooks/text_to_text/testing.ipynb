{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base_model import TextGenerationModelLoader, ImageToTextModelLoader, VQAModelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama\n",
    "\n",
    "\n",
    "- **[meta-llama/Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7B)**: Компактная версия, подходит для задач с ограниченными ресурсами.\n",
    "- **[meta-llama/Llama-2-13B](https://huggingface.co/meta-llama/Llama-2-13B)**: Средняя модель, обеспечивает баланс между производительностью и ресурсами.\n",
    "- **[meta-llama/Llama-2-70B](https://huggingface.co/meta-llama/Llama-2-70B)**: Модель с высокой точностью, требует значительных ресурсов.\n",
    "- **[meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)**: Обучена для задач инструкций, подходит для генерации текста и вопросов.\n",
    "- **[meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)**: Поддерживает мультимодальные задачи (текст + изображение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование Text Generation Model\n",
    "print(\"Тестирование генерации текста:\")\n",
    "text_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Замените на актуальное имя модели\n",
    "text_model = TextGenerationModelLoader(model_name=text_model_name)\n",
    "\n",
    "text_prompt = \"Explain the main benefits of using Llama models for language tasks.\"\n",
    "generated_text = text_model.generate_text(text_prompt)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Тестирование Image to Text Model\n",
    "print(\"\\nТестирование генерации описания изображения:\")\n",
    "image_model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"  # Замените на актуальное имя модели\n",
    "image_model = ImageToTextModelLoader(model_name=image_model_name)\n",
    "\n",
    "image_path = \"path_to_your_image.jpg\"  # Укажите путь к тестовому изображению\n",
    "image_description = image_model.generate_image_description(image_path)\n",
    "\n",
    "print(\"Описание изображения:\")\n",
    "print(image_description)\n",
    "\n",
    "# Тестирование VQA Model\n",
    "print(\"\\nТестирование визуально-вопросно-ответной модели (VQA):\")\n",
    "vqa_model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"  # Замените на актуальное имя модели\n",
    "vqa_model = VQAModelLoader(model_name=vqa_model_name)\n",
    "\n",
    "vqa_question = \"What objects can you see in the image?\"\n",
    "vqa_answer = vqa_model.answer_question(image_path, vqa_question)\n",
    "\n",
    "print(\"Ответ на вопрос по изображению:\")\n",
    "print(vqa_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFA\n",
    "\n",
    "- **[OFA-Tiny](https://huggingface.co/OFA-Sys/OFA-tiny)**: Легкая версия, оптимизированная для экономии памяти и скорости.\n",
    "- **[OFA-Medium](https://huggingface.co/OFA-Sys/OFA-medium)**: Сбалансированная модель для задач с ограниченными ресурсами.\n",
    "- **[OFA-Base](https://huggingface.co/OFA-Sys/OFA-base)**: Подходит для большинства приложений, обеспечивая хорошее соотношение производительности и ресурсов.\n",
    "- **[OFA-Large](https://huggingface.co/OFA-Sys/OFA-large)**: Повышенная производительность, подходит для более требовательных задач.\n",
    "- **[OFA-Huge](https://huggingface.co/OFA-Sys/OFA-huge)**: Модель с максимальной точностью и ресурсозатратностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование Text Generation Model с OFA\n",
    "print(\"Тестирование генерации текста:\")\n",
    "text_model_name = \"OFA-Sys/ofa-tiny\"  # Выберите модель OFA для генерации текста\n",
    "text_model = TextGenerationModelLoader(model_name=text_model_name)\n",
    "\n",
    "text_prompt = \"Explain the potential applications of the OFA model in AI research.\"\n",
    "generated_text = text_model.generate_text(text_prompt)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Тестирование Image to Text Model с OFA\n",
    "print(\"\\nТестирование генерации описания изображения:\")\n",
    "image_model_name = \"OFA-Sys/ofa-tiny\"  # Выберите модель OFA для задачи image-to-text\n",
    "image_model = ImageToTextModelLoader(model_name=image_model_name)\n",
    "\n",
    "image_path = \"path_to_your_image.jpg\"  # Укажите путь к тестовому изображению\n",
    "image_description = image_model.generate_image_description(image_path)\n",
    "\n",
    "print(\"Описание изображения:\")\n",
    "print(image_description)\n",
    "\n",
    "# Тестирование VQA Model с OFA\n",
    "print(\"\\nТестирование визуально-вопросно-ответной модели (VQA):\")\n",
    "vqa_model_name = \"OFA-Sys/ofa-tiny\"  # Выберите модель OFA для задачи VQA\n",
    "vqa_model = VQAModelLoader(model_name=vqa_model_name)\n",
    "\n",
    "vqa_question = \"What objects are visible in the image?\"\n",
    "vqa_answer = vqa_model.answer_question(image_path, vqa_question)\n",
    "\n",
    "print(\"Ответ на вопрос по изображению:\")\n",
    "print(vqa_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP\n",
    "\n",
    "- **[BLIP-base](https://huggingface.co/Salesforce/blip-image-captioning-base)**: Базовая версия модели для описания изображений.\n",
    "- **[BLIP-large](https://huggingface.co/Salesforce/blip-image-captioning-large)**: Более мощная версия для описания изображений.\n",
    "- **[BLIP-VQA-base](https://huggingface.co/Salesforce/blip-vqa-base)**: Базовая версия модели для визуально-вопросно-ответных задач.\n",
    "- **[BLIP-VQA-large](https://huggingface.co/Salesforce/blip-vqa-large)**: Улучшенная версия модели для VQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестирование генерации описания изображения:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\env_ml\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при загрузке модели: 'latin-1' codec can't encode characters in position 38-45: ordinal not in range(256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Пайплайн не инициализирован.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m image_model \u001b[38;5;241m=\u001b[39m ImageToTextModelLoader(model_name\u001b[38;5;241m=\u001b[39mimage_model_name)\n\u001b[0;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_your_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Укажите путь к тестовому изображению\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m image_description \u001b[38;5;241m=\u001b[39m \u001b[43mimage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_image_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОписание изображения:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_description)\n",
      "File \u001b[1;32md:\\Projects\\llama3.2_python_pipline\\models\\base_model.py:147\u001b[0m, in \u001b[0;36mImageToTextModelLoader.generate_image_description\u001b[1;34m(self, image_path)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Генерирует текстовое описание для изображения.\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПайплайн не инициализирован.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Загрузка изображения\u001b[39;00m\n\u001b[0;32m    150\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Пайплайн не инициализирован."
     ]
    }
   ],
   "source": [
    "# Тестирование Image to Text Model с BLIP\n",
    "print(\"Тестирование генерации описания изображения:\")\n",
    "image_model_name = \"Salesforce/blip-image-captioning-base\"  # Выберите модель BLIP для задачи image-to-text\n",
    "image_model = ImageToTextModelLoader(model_name=image_model_name)\n",
    "\n",
    "image_path = \"path_to_your_image.jpg\"  # Укажите путь к тестовому изображению\n",
    "image_description = image_model.generate_image_description(image_path)\n",
    "\n",
    "print(\"Описание изображения:\")\n",
    "print(image_description)\n",
    "\n",
    "# Тестирование VQA Model с BLIP\n",
    "print(\"\\nТестирование визуально-вопросно-ответной модели (VQA):\")\n",
    "vqa_model_name = \"Salesforce/blip-vqa-base\"  # Выберите модель BLIP для задачи VQA\n",
    "vqa_model = VQAModelLoader(model_name=vqa_model_name)\n",
    "\n",
    "vqa_question = \"What is happening in the image?\"\n",
    "vqa_answer = vqa_model.answer_question(image_path, vqa_question)\n",
    "\n",
    "print(\"Ответ на вопрос по изображению:\")\n",
    "print(vqa_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIT\n",
    "\n",
    "- **[GIT-base](https://huggingface.co/microsoft/git-base)**: Базовая версия модели для генерации описаний изображений.\n",
    "- **[GIT-large](https://huggingface.co/microsoft/git-large)**: Улучшенная версия модели для генерации описаний изображений с более высоким качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование Image to Text Model с GIT\n",
    "print(\"Тестирование генерации описания изображения с GIT:\")\n",
    "image_model_name = \"microsoft/git-base\"  # Выберите модель GIT для задачи image-to-text\n",
    "image_model = ImageToTextModelLoader(model_name=image_model_name)\n",
    "\n",
    "image_path = \"path_to_your_image.jpg\"  # Укажите путь к тестовому изображению\n",
    "image_description = image_model.generate_image_description(image_path)\n",
    "\n",
    "print(\"Описание изображения:\")\n",
    "print(image_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5\n",
    "\n",
    "- **[T5-small](https://huggingface.co/t5-small)**: Легкая версия модели T5, оптимизированная для быстрого выполнения.\n",
    "- **[T5-base](https://huggingface.co/t5-base)**: Базовая версия T5 с более высокой точностью, чем T5-small.\n",
    "- **[T5-large](https://huggingface.co/t5-large)**: Улучшенная версия модели для более точной генерации текста.\n",
    "- **[T5-3B](https://huggingface.co/t5-3b)**: Версия с 3 миллиардами параметров, которая обеспечивает высокий уровень производительности.\n",
    "- **[T5-11B](https://huggingface.co/t5-11b)**: Наиболее мощная версия модели T5 с 11 миллиардами параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование генерации текста с T5\n",
    "print(\"Тестирование генерации текста с T5:\")\n",
    "text_model_name = \"t5-base\"  # Выберите модель T5 для задачи генерации текста\n",
    "text_model = TextGenerationModelLoader(model_name=text_model_name)\n",
    "\n",
    "prompt = \"translate English to French: The weather is nice today.\"  # Пример запроса для перевода\n",
    "generated_text = text_model.generate_text(prompt)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-based generators\n",
    "\n",
    "Хотя **BERT** изначально не предназначен для генерации текста, существуют его версии, адаптированные для таких задач, как суммаризация и генерация ответов. Эти модели основаны на BERT и других трансформерах, модифицированных для генерации текста:\n",
    "\n",
    "- **[BERT2BERT](https://huggingface.co/google/bert2bert_L-24_wmt_de_en)**: Модель для генерации текстов и перевода, основанная на архитектуре BERT для задач перевода.\n",
    "- **[CTRL](https://huggingface.co/salesforce/ctrl)**: Модель на основе BERT, предназначенная для более контролируемой генерации текста.\n",
    "- **[DistilBERT for Summarization](https://huggingface.co/sshleifer/distilbart-cnn-12-6)**: Легкая модель для задачи суммаризации, основанная на BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование генерации текста с BERT-based моделью\n",
    "print(\"Тестирование генерации текста с BERT-based генератором:\")\n",
    "bert_model_name = \"google/bert2bert_L-24_wmt_de_en\"  # Выберите BERT-based модель, например, для перевода\n",
    "text_model = TextGenerationModelLoader(model_name=bert_model_name)\n",
    "\n",
    "prompt = \"translate English to German: The machine learning model is highly accurate.\"  # Пример запроса для перевода\n",
    "generated_text = text_model.generate_text(prompt)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
